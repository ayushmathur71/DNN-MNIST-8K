{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-15T14:40:44.825338Z","iopub.execute_input":"2024-03-15T14:40:44.826179Z","iopub.status.idle":"2024-03-15T14:40:44.830867Z","shell.execute_reply.started":"2024-03-15T14:40:44.826145Z","shell.execute_reply":"2024-03-15T14:40:44.830001Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Adding rotation to the transforms\n# Train Phase transformations\ntrain_transforms2 = transforms.Compose([\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n\n# Test Phase transformations\ntest_transforms2 = transforms.Compose([\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n\ntrain = datasets.MNIST('./data', train=True, download=True, transform=train_transforms2)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms2)\n\n\n#setting manual seed\nSEED=1\n\n#CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n#For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n  torch.cuda.manual_seed(SEED)\n\n#dataloader arguments\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n#train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n#test loader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T14:43:41.652360Z","iopub.execute_input":"2024-03-15T14:43:41.653051Z","iopub.status.idle":"2024-03-15T14:43:42.999211Z","shell.execute_reply.started":"2024-03-15T14:43:41.653018Z","shell.execute_reply":"2024-03-15T14:43:42.998518Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 99467945.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 50684390.72it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1648877/1648877 [00:00<00:00, 22755181.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 7532830.67it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"CUDA Available? True\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar=tqdm(train_loader)\n  correct=0\n  processed=0\n\n  for batch_idx, (data, target) in enumerate(pbar):\n    #get samples\n    data, target = data.to(device), target.to(device)\n\n    #Init\n    optimizer.zero_grad()\n\n    #Predict\n    y_pred=model(data)\n\n    #Calculate loss\n    loss=F.nll_loss(y_pred,target)\n    train_losses.append(loss)\n\n    #Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    #Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n    correct+=pred.eq(target.view_as(pred)).sum().item()\n    processed+= len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy = {100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n  model.eval()\n  test_loss = 0\n  correct = 0\n\n  with torch.no_grad():\n    for data, target in test_loader:\n      data, target = data.to(device), target.to(device)\n      output = model(data)\n      test_loss += F.nll_loss(output, target, reduction='sum').item() #sum up batch loss\n      pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n      correct += pred.eq(target.view_as(pred)).sum().item()\n\n  test_loss /=len(test_loader.dataset)\n  test_losses.append(test_loss)\n\n  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n      test_loss, correct, len(test_loader.dataset),\n      100*correct/len(test_loader.dataset)))\n  test_acc.append(100* correct/len(test_loader.dataset))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T14:44:57.689130Z","iopub.execute_input":"2024-03-15T14:44:57.689765Z","iopub.status.idle":"2024-03-15T14:44:57.702688Z","shell.execute_reply.started":"2024-03-15T14:44:57.689730Z","shell.execute_reply":"2024-03-15T14:44:57.701557Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Model\n**Target -**\n\n1. Add Global Average Pooling\n2. Add Regularization (Dropouts)\n3. Add Global Average Pooling","metadata":{}},{"cell_type":"code","source":"\ndropout_value = 0.02\nclass Net5(nn.Module):\n    def __init__(self):\n        super(Net5, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 26 ; RF=3\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 24 ; RF=5\n\n        # TRANSITION BLOCK 1\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n        ) # output_size = 24 ; RF_in + (k-1)J_in = 5 + (1-1) = 5 ; J_in = 1\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12 ; RF=5+(2-1)1 = 6 ; J_out=2\n\n        # CONVOLUTION BLOCK 2\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 12 ; RF=6+(3-1)*2=10 ; J_Out=2\n        \n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n            nn.ReLU(),            \n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 12 ; RF=10+(3-1)*2=14 ; J_Out=2\n        \n        # TRANSITION BLOCK 2\n        self.pool2 = nn.MaxPool2d(2, 2) # output_size = 6 ; RF=14+(2-1)1 = 15 ; J_out=2\n        \n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n        ) # output_size = 12 ; RF_in + (k-1)J_in = 14 + (1-1) = 14 ; J_in = 1\n       \n        \n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=1, bias=False),\n            nn.ReLU(),            \n            nn.BatchNorm2d(8),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6 ; RF=10+(3-1)*2=14 ; J_Out=2\n        \n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=1, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(8),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6 ; RF=18+(3-1)*2=22\n        \n        # OUTPUT BLOCK\n        self.gap = nn.Sequential(\n            nn.AvgPool2d(kernel_size=5)\n        ) # output_size = 1\n\n        self.convblock9 = nn.Sequential(\n            nn.Conv2d(in_channels=8, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            # nn.BatchNorm2d(10),\n            # nn.ReLU(),\n            # nn.Dropout(dropout_value)\n        )\n\n\n        self.dropout = nn.Dropout(dropout_value)\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        \n        x = self.convblock3(x)\n        x = self.pool1(x)\n        \n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        \n        x = self.convblock6(x)\n        x = self.pool2(x)\n        \n        x = self.convblock7(x)\n        x = self.convblock8(x)\n        x = self.gap(x)        \n        x = self.convblock9(x)\n\n        x = x.view(-1,10)\n        return F.log_softmax(x, dim=-1)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T14:47:06.356627Z","iopub.execute_input":"2024-03-15T14:47:06.357655Z","iopub.status.idle":"2024-03-15T14:47:06.377570Z","shell.execute_reply.started":"2024-03-15T14:47:06.357615Z","shell.execute_reply":"2024-03-15T14:47:06.376755Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nmodel = Net5().to(device)\nsummary(model, input_size=(1, 28, 28))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T14:47:20.848475Z","iopub.execute_input":"2024-03-15T14:47:20.848855Z","iopub.status.idle":"2024-03-15T14:47:38.923853Z","shell.execute_reply.started":"2024-03-15T14:47:20.848827Z","shell.execute_reply":"2024-03-15T14:47:38.922701Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 26, 26]             144\n              ReLU-2           [-1, 16, 26, 26]               0\n       BatchNorm2d-3           [-1, 16, 26, 26]              32\n           Dropout-4           [-1, 16, 26, 26]               0\n            Conv2d-5           [-1, 16, 24, 24]           2,304\n              ReLU-6           [-1, 16, 24, 24]               0\n       BatchNorm2d-7           [-1, 16, 24, 24]              32\n           Dropout-8           [-1, 16, 24, 24]               0\n            Conv2d-9           [-1, 10, 24, 24]             160\n        MaxPool2d-10           [-1, 10, 12, 12]               0\n           Conv2d-11           [-1, 16, 10, 10]           1,440\n             ReLU-12           [-1, 16, 10, 10]               0\n      BatchNorm2d-13           [-1, 16, 10, 10]              32\n          Dropout-14           [-1, 16, 10, 10]               0\n           Conv2d-15           [-1, 16, 10, 10]           2,304\n             ReLU-16           [-1, 16, 10, 10]               0\n      BatchNorm2d-17           [-1, 16, 10, 10]              32\n          Dropout-18           [-1, 16, 10, 10]               0\n           Conv2d-19            [-1, 8, 10, 10]             128\n        MaxPool2d-20              [-1, 8, 5, 5]               0\n           Conv2d-21              [-1, 8, 5, 5]             576\n             ReLU-22              [-1, 8, 5, 5]               0\n      BatchNorm2d-23              [-1, 8, 5, 5]              16\n          Dropout-24              [-1, 8, 5, 5]               0\n           Conv2d-25              [-1, 8, 5, 5]             576\n             ReLU-26              [-1, 8, 5, 5]               0\n      BatchNorm2d-27              [-1, 8, 5, 5]              16\n          Dropout-28              [-1, 8, 5, 5]               0\n        AvgPool2d-29              [-1, 8, 1, 1]               0\n           Conv2d-30             [-1, 10, 1, 1]              80\n================================================================\nTotal params: 7,872\nTrainable params: 7,872\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.78\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.82\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Net5().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 15\n\nfor epoch in range(EPOCHS):\n    print('EPOCH: ',epoch+1)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T14:48:58.112187Z","iopub.execute_input":"2024-03-15T14:48:58.113081Z","iopub.status.idle":"2024-03-15T14:51:12.709387Z","shell.execute_reply.started":"2024-03-15T14:48:58.113047Z","shell.execute_reply":"2024-03-15T14:51:12.708078Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"EPOCH:  1\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.14398376643657684 Batch_id=468 Accuracy = 86.39: 100%|██████████| 469/469 [00:08<00:00, 56.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.1377, Accuracy: 9638/10000 (96.38%)\n\nEPOCH:  2\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.1405966430902481 Batch_id=468 Accuracy = 97.17: 100%|██████████| 469/469 [00:07<00:00, 60.15it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0671, Accuracy: 9806/10000 (98.06%)\n\nEPOCH:  3\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.07972168922424316 Batch_id=468 Accuracy = 97.91: 100%|██████████| 469/469 [00:08<00:00, 58.30it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0540, Accuracy: 9840/10000 (98.40%)\n\nEPOCH:  4\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.016583170741796494 Batch_id=468 Accuracy = 98.33: 100%|██████████| 469/469 [00:07<00:00, 62.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0475, Accuracy: 9851/10000 (98.51%)\n\nEPOCH:  5\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.1383960098028183 Batch_id=468 Accuracy = 98.53: 100%|██████████| 469/469 [00:07<00:00, 61.61it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0388, Accuracy: 9878/10000 (98.78%)\n\nEPOCH:  6\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.05339514836668968 Batch_id=468 Accuracy = 98.63: 100%|██████████| 469/469 [00:07<00:00, 61.99it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0368, Accuracy: 9885/10000 (98.85%)\n\nEPOCH:  7\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.07216107100248337 Batch_id=468 Accuracy = 98.69: 100%|██████████| 469/469 [00:07<00:00, 61.74it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0429, Accuracy: 9866/10000 (98.66%)\n\nEPOCH:  8\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.04203372821211815 Batch_id=468 Accuracy = 98.81: 100%|██████████| 469/469 [00:07<00:00, 61.00it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0338, Accuracy: 9897/10000 (98.97%)\n\nEPOCH:  9\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.020637812092900276 Batch_id=468 Accuracy = 98.95: 100%|██████████| 469/469 [00:07<00:00, 59.67it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0320, Accuracy: 9898/10000 (98.98%)\n\nEPOCH:  10\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.03204457834362984 Batch_id=468 Accuracy = 98.96: 100%|██████████| 469/469 [00:08<00:00, 57.95it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0341, Accuracy: 9895/10000 (98.95%)\n\nEPOCH:  11\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.010715226642787457 Batch_id=468 Accuracy = 99.03: 100%|██████████| 469/469 [00:07<00:00, 60.75it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0319, Accuracy: 9907/10000 (99.07%)\n\nEPOCH:  12\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.04326753318309784 Batch_id=468 Accuracy = 99.10: 100%|██████████| 469/469 [00:07<00:00, 60.05it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0313, Accuracy: 9906/10000 (99.06%)\n\nEPOCH:  13\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.06951046735048294 Batch_id=468 Accuracy = 99.14: 100%|██████████| 469/469 [00:07<00:00, 61.16it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0358, Accuracy: 9894/10000 (98.94%)\n\nEPOCH:  14\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.07219089567661285 Batch_id=468 Accuracy = 99.11: 100%|██████████| 469/469 [00:08<00:00, 58.21it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0281, Accuracy: 9916/10000 (99.16%)\n\nEPOCH:  15\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.0371481254696846 Batch_id=468 Accuracy = 99.19: 100%|██████████| 469/469 [00:07<00:00, 61.23it/s]   \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0278, Accuracy: 9910/10000 (99.10%)\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Results & Analysis\n\n**Results:**\n\nParameters: 7.8k\n\nBest Train Accuracy: 99.19%\n\nBest Test Accuracy: 99.16%\n\n**Analysis:**\n\nOver-fitting is reduced after adding dropout value & GAP layer\nBut 99.4 still cannot be reached even if model is pushed further\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}